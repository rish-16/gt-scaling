import json, math
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

font = {'family' : 'normal',
        'size'   : 17}

matplotlib.rc('font', **font)
matplotlib.rc('legend', **{"loc": "upper left"})

# ALL TIMINGS ONLY HAVE Atom+LapPE encoders
transformer_inferences1 = {
    13: 0.021492958068847656,
    14: 0.021889925003051758,
    # 15: 0.8974058628082275,
    16: 0.02248859405517578,
    17: 0.02243518829345703,
    18: 0.022876501083374023,
    19: 0.026668548583984375,
    20: 0.022913455963134766,
    21: 0.023674726486206055,
    22: 0.023288488388061523,
    23: 0.023489952087402344,
    24: 0.023614168167114258,
    25: 0.024321556091308594,
    26: 0.023944854736328125,
    27: 0.024423599243164062,
    28: 0.02703404426574707,
    29: 0.0246584415435791,
    30: 0.024599313735961914,
    31: 0.027340173721313477,
    32: 0.02505636215209961,
    33: 0.025157451629638672,
    34: 0.025924205780029297,
    35: 0.028073549270629883
}

transformer_inferences2 = {
    13: 0.021201610565185547,
    14: 0.02138042449951172,
    # 15: 0.8882522583007812,
    16: 0.021914958953857422,
    17: 0.021843671798706055,
    18: 0.021974563598632812,
    19: 0.02254486083984375,
    20: 0.022498369216918945,
    21: 0.022711515426635742,
    22: 0.022804737091064453,
    23: 0.022800922393798828,
    24: 0.02350592613220215,
    25: 0.023587703704833984,
    26: 0.023636579513549805,
    27: 0.02385735511779785,
    28: 0.026776552200317383,
    29: 0.02784132957458496,
    30: 0.024341106414794922,
    31: 0.027054309844970703,
    32: 0.024399757385253906,
    33: 0.024524211883544922,
    34: 0.025383472442626953,
    35: 0.02763080596923828
}

transformer_inferences3 = {
    13: 0.021007776260375977,
    14: 0.020985126495361328,
    # 15: 0.8889889717102051,
    16: 0.021576881408691406,
    17: 0.02582383155822754,
    18: 0.021978139877319336,
    19: 0.0222015380859375,
    20: 0.02215576171875,
    21: 0.022709369659423828,
    22: 0.022669315338134766,
    23: 0.022730350494384766,
    24: 0.02295684814453125,
    25: 0.023218870162963867,
    26: 0.023705005645751953,
    27: 0.023502111434936523,
    28: 0.026354312896728516,
    29: 0.024196147918701172,
    30: 0.024160146713256836,
    31: 0.026738405227661133,
    32: 0.0243074893951416,
    33: 0.024328231811523438,
    34: 0.025281906127929688,
    35: 0.02753305435180664
}

performer_inferences1 = {
1: 0.05425095558166504,
 14: 0.057489871978759766,
 15: 0.06299090385437012,
 16: 0.061998844146728516,
 17: 0.058785200119018555,
 18: 0.05951118469238281,
 19: 0.060259342193603516,
 20: 0.06083512306213379,
 21: 0.061995506286621094,
 22: 0.06556129455566406,
 23: 0.06431150436401367,
 24: 0.06595897674560547,
 25: 0.06916260719299316,
 26: 0.9413666725158691,
 27: 0.06744766235351562,
 28: 0.0676581859588623,
 29: 0.06867694854736328,
 30: 0.07247471809387207,
 31: 0.07038736343383789,
 32: 0.0711662769317627,
 34: 0.07294631004333496,
 35: 0.0738368034362793
}

performer_inferences2 = {
    
}

performer_inferences3 = {
    
}

bigbird_inferences1 = {
    10: 0.06829190254211426,
    11: 0.07210540771484375,
    12: 0.06725907325744629,
    13: 0.07325243949890137,
    14: 0.07448148727416992,
    15: 0.07395124435424805,
    16: 0.0757150650024414,
    17: 0.07570433616638184,
    # 18: 0.953228235244751,
    19: 0.07775115966796875,
    20: 0.07759284973144531,
    21: 0.07782602310180664,
    22: 0.07945632934570312,
    23: 0.07993316650390625,
    24: 0.08675479888916016,
    25: 0.08792972564697266,
    26: 0.08866524696350098,
    27: 0.08935165405273438,
    28: 0.09382510185241699,
    29: 0.09098696708679199,
    30: 0.09073948860168457,
    31: 0.09333181381225586,
    32: 0.10935449600219727,
    33: 0.09752202033996582,
    34: 0.08834338188171387,
    35: 0.08857846260070801,
    36: 0.08878540992736816,
    37: 0.09459996223449707,
    38: 0.0906825065612793,
    39: 0.09165143966674805,
    40: 0.09864425659179688,
    41: 0.09879517555236816,
    42: 0.09480643272399902,
    43: 0.0979771614074707,
    44: 0.10240912437438965,
    45: 0.09818267822265625,
    46: 0.10363078117370605,
    48: 0.10150551795959473,
    49: 0.10466790199279785,
    50: 0.10733938217163086,
    51: 0.1082301139831543
}

bigbird_inferences2 = {
    10: 0.07253885269165039,
    11: 0.0736391544342041,
    12: 0.07227826118469238,
    13: 0.0798807144165039,
    14: 0.07990264892578125,
    15: 0.0796353816986084,
    16: 0.08222723007202148,
    17: 0.08250927925109863,
    # 18: 0.9986498355865479,
    19: 0.08270978927612305,
    20: 0.08441162109375,
    21: 0.08327078819274902,
    22: 0.08446002006530762,
    23: 0.08500194549560547,
    24: 0.08472847938537598,
    25: 0.09403467178344727,
    26: 0.09511399269104004,
    27: 0.09260034561157227,
    28: 0.09790849685668945,
    29: 0.10083150863647461,
    30: 0.0990755558013916,
    31: 0.10206103324890137,
    32: 0.10535526275634766,
    33: 0.10240578651428223,
    34: 0.09576153755187988,
    35: 0.0969851016998291,
    36: 0.09553027153015137,
    37: 0.09796714782714844,
    38: 0.09793663024902344,
    39: 0.09849309921264648,
    40: 0.10187768936157227,
    41: 0.10073208808898926,
    42: 0.10262513160705566,
    43: 0.10562634468078613,
    44: 0.10426950454711914,
    45: 0.10526871681213379,
    46: 0.11015629768371582,
    48: 0.10840129852294922,
    49: 0.11053895950317383,
    50: 0.11199188232421875,
    51: 0.11071419715881348
}

bigbird_inferences3 = {
    10: 0.05961275100708008,
    11: 0.058423519134521484,
    12: 0.05726432800292969,
    13: 0.0689239501953125,
    14: 0.06479573249816895,
    15: 0.06435894966125488,
    16: 0.06703448295593262,
    17: 0.06704902648925781,
    # 18: 0.9388813972473145, 
    19: 0.06779026985168457,
    20: 0.0677497386932373,
    21: 0.06758594512939453,
    22: 0.06838726997375488,
    23: 0.06885814666748047,
    24: 0.07159638404846191,
    25: 0.07726263999938965,
    26: 0.07731270790100098,
    27: 0.07663154602050781,
    28: 0.08025598526000977,
    29: 0.08089685440063477,
    30: 0.08426046371459961,
    31: 0.08324766159057617,
    32: 0.08376073837280273,
    33: 0.08277153968811035,
    34: 0.07505083084106445,
    35: 0.07589983940124512,
    36: 0.07523608207702637,
    37: 0.07657980918884277,
    38: 0.07659268379211426,
    39: 0.07617759704589844,
    40: 0.07806801795959473,
    41: 0.08188295364379883,
    42: 0.07755661010742188,
    43: 0.0803828239440918,
    44: 0.07973027229309082,
    45: 0.07917141914367676,
    46: 0.08440661430358887,
    48: 0.08081984519958496,
    49: 0.08222770690917969,
    50: 0.08336305618286133,
    51: 0.08169245719909668
}

transformer_x = list(transformer_inferences1.keys())
transformer_y1 = list(transformer_inferences1.values())
transformer_y2 = list(transformer_inferences2.values())
transformer_y3 = list(transformer_inferences3.values())

transformer_y = []
for y1, y2, y3 in zip(transformer_y1, transformer_y2, transformer_y3):
    transformer_y.append([y1, y2, y3])
transformer_y = np.asarray(transformer_y)
transformer_y_avg = transformer_y.mean(axis=1)
transformer_y_std = transformer_y.std(axis=1)

# print (transformer_y_avg)
# print (sum([0.02037954330444336, 0.020636796951293945, 0.020218849182128906]) / 3)
# print (transformer_y_avg[0])

performer_x = list(performer_inferences1.keys())
performer_y1 = list(performer_inferences1.values())
performer_y2 = list(performer_inferences2.values())
performer_y3 = list(performer_inferences3.values())

performer_y = []
for y1, y2, y3 in zip(performer_y1, performer_y2, performer_y3):
    performer_y.append([y1, y2, y3])
performer_y = np.asarray(performer_y)
performer_y_avg = performer_y.mean(axis=1)
performer_y_std = performer_y.std(axis=1)

bigbird_x = list(bigbird_inferences1.keys())
bigbird_y1 = list(bigbird_inferences1.values())
bigbird_y2 = list(bigbird_inferences2.values())
bigbird_y3 = list(bigbird_inferences3.values()) 

bigbird_y = []
for y1, y2, y3 in zip(bigbird_y1, bigbird_y2, bigbird_y3):
    bigbird_y.append([y1, y2, y3])
bigbird_y = np.asarray(bigbird_y)
bigbird_y_avg = bigbird_y.mean(axis=1)
bigbird_y_std = bigbird_y.std(axis=1)

fig = plt.figure()
ax = fig.add_subplot(111)

# xint = range(min(transformer_x), math.ceil(max(transformer_x))+1)
# matplotlib.pyplot.xticks(xint)

plt.plot(transformer_x, transformer_y_avg, color="red", label="GPS-Transformer-M", markersize=10)
plt.fill_between(
    transformer_x,
    np.asarray(transformer_y_avg) - np.asarray(transformer_y_std), 
    np.asarray(transformer_y_avg) + np.asarray(transformer_y_std),
    alpha=0.3,
    color="red"
)

plt.plot(performer_x, performer_y_avg, color="blue", label="GPS-Performer-M", markersize=10)
plt.fill_between(
    performer_x,
    np.asarray(performer_y_avg) - np.asarray(performer_y_std),
    np.asarray(performer_y_avg) + np.asarray(performer_y_std),
    alpha=0.3,
    color="blue"
)

plt.plot(bigbird_x, bigbird_y_avg, color="green", label="GPS-BigBird-M")
plt.fill_between(
    bigbird_x,
    np.asarray(bigbird_y_avg) - np.asarray(bigbird_y_std),
    np.asarray(bigbird_y_avg) + np.asarray(bigbird_y_std),
    alpha=0.3,
    color="green"
)

plt.xlabel("Number of atoms")
plt.ylabel("Batch Inference Runtime (B = 128) / sec")
plt.legend()
plt.grid(linestyle="dashed")
plt.show()

fig.savefig("figures/scaling/pcqm4m_variant_scaling_graph_size.pdf", dpi=400, bbox_inches='tight')