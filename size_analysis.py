import json, math
import numpy as np
import matplotlib.pyplot as plt
import matplotlib

font = {'family' : 'normal',
        'size'   : 17}

matplotlib.rc('font', **font)
matplotlib.rc('legend', **{"loc": "upper left"})

# ALL TIMINGS ONLY HAVE Atom+LapPE encoders
transformer_inferences1 = {
    10: 0.0210878849029541,
    11: 0.02142810821533203,
    12: 0.021634817123413086,
    13: 0.020940542221069336,
    14: 0.021176815032958984,
    15: 0.0219576358795166,
    16: 0.021819591522216797,
    17: 0.024213790893554688,
    # 18: 0.8587734699249268,
    19: 0.024828672409057617,
    20: 0.02355027198791504,
    21: 0.02503371238708496,
    22: 0.024462461471557617,
    23: 0.02521347999572754,
    24: 0.025012969970703125,
    25: 0.026360750198364258,
    26: 0.026955127716064453,
    27: 0.0258941650390625,
    28: 0.026270389556884766,
    29: 0.028356075286865234,
    30: 0.029010534286499023,
    31: 0.028917789459228516,
    32: 0.029778718948364258,
    33: 0.030831336975097656,
    34: 0.03106212615966797,
    35: 0.03188824653625488,
    36: 0.03206610679626465,
    37: 0.03287458419799805,
    38: 0.033350467681884766,
    39: 0.03499245643615723,
    40: 0.03728532791137695,
    41: 0.03594350814819336,
    42: 0.036856889724731445,
    43: 0.03744864463806152,
    44: 0.0368039608001709,
    45: 0.03852510452270508,
    46: 0.039437294006347656,
    48: 0.038602352142333984,
    49: 0.039053916931152344,
    50: 0.04143929481506348,
    51: 0.04032778739929199
}

transformer_inferences2 = {
    10: 0.02608633041381836,
    11: 0.02634572982788086,
    12: 0.029651165008544922,
    13: 0.026338815689086914,
    14: 0.026245594024658203,
    15: 0.02657628059387207,
    16: 0.02678966522216797,
    17: 0.028587818145751953,
    # 18: 0.8746860027313232,
    19: 0.02904820442199707,
    20: 0.027483224868774414,
    21: 0.029231786727905273,
    22: 0.028432130813598633,
    23: 0.02855825424194336,
    24: 0.02861189842224121,
    25: 0.02907562255859375,
    26: 0.030395030975341797,
    27: 0.028773069381713867,
    28: 0.03264808654785156,
    29: 0.030930280685424805,
    30: 0.0313413143157959,
    31: 0.031176090240478516,
    32: 0.032811880111694336,
    33: 0.032517433166503906,
    34: 0.032851457595825195,
    35: 0.03380084037780762,
    36: 0.03363776206970215,
    37: 0.03423023223876953,
    38: 0.03478741645812988,
    39: 0.036330461502075195,
    40: 0.035921573638916016,
    41: 0.03703141212463379,
    42: 0.038382768630981445,
    43: 0.0383753776550293,
    44: 0.03789973258972168,
    45: 0.03969979286193848,
    46: 0.04051852226257324,
    48: 0.0397801399230957,
    49: 0.04022812843322754,
    50: 0.0427088737487793,
    51: 0.04505157470703125
}

transformer_inferences3 = {
    10: 0.021399497985839844,
    11: 0.021289825439453125,
    12: 0.02173590660095215,
    13: 0.0214235782623291,
    14: 0.02150583267211914,
    15: 0.022142648696899414,
    16: 0.022140026092529297,
    17: 0.02397751808166504,
    # 18: 0.861675500869751,
    19: 0.024582862854003906,
    20: 0.023051023483276367,
    21: 0.024739980697631836,
    22: 0.024602174758911133,
    23: 0.025003910064697266,
    24: 0.024953126907348633,
    25: 0.026077985763549805,
    26: 0.026868343353271484,
    27: 0.02602696418762207,
    28: 0.026267290115356445,
    29: 0.028423547744750977,
    30: 0.029098033905029297,
    31: 0.029093027114868164,
    32: 0.029384136199951172,
    33: 0.030832529067993164,
    34: 0.031035184860229492,
    35: 0.03159475326538086,
    36: 0.035269975662231445,
    37: 0.033156394958496094,
    38: 0.033516883850097656,
    39: 0.03514218330383301,
    40: 0.03491854667663574,
    41: 0.035771846771240234,
    42: 0.037122488021850586,
    43: 0.037397146224975586,
    44: 0.036820411682128906,
    45: 0.038767337799072266,
    46: 0.039504289627075195,
    48: 0.03876757621765137,
    49: 0.039266347885131836,
    50: 0.04142427444458008,
    51: 0.04053783416748047
}

performer_inferences1 = {
    10: 0.04695487022399902,
    11: 0.05025959014892578,
    12: 0.05095505714416504,
    13: 0.0530853271484375,
    14: 0.0543515682220459,
    15: 0.06136512756347656,
    16: 0.06025290489196777,
    17: 0.05770730972290039,
    # 18: 0.8922617435455322,
    19: 0.057485103607177734,
    20: 0.05975508689880371,
    21: 0.05968308448791504,
    22: 0.06055450439453125,
    23: 0.06184101104736328,
    24: 0.06273937225341797,
    25: 0.06486058235168457,
    26: 0.0663905143737793,
    27: 0.06666302680969238,
    28: 0.06899642944335938,
    29: 0.07161402702331543,
    30: 0.0724635124206543,
    31: 0.07395648956298828,
    32: 0.07489180564880371,
    33: 0.07736825942993164,
    34: 0.07808256149291992,
    35: 0.0794978141784668,
    36: 0.08106112480163574,
    37: 0.08230876922607422,
    38: 0.08289694786071777,
    39: 0.08552908897399902,
    40: 0.0869438648223877,
    41: 0.08825302124023438,
    42: 0.09113025665283203,
    43: 0.09199261665344238,
    44: 0.09290909767150879,
    45: 0.09447002410888672,
    46: 0.09577703475952148,
    48: 0.09956860542297363,
    49: 0.09838676452636719,
    50: 0.10131549835205078,
    51: 0.10115766525268555
}

performer_inferences2 = {
    10: 0.04689836502075195,
    11: 0.05005693435668945,
    12: 0.051149845123291016,
    13: 0.05290389060974121,
    14: 0.054366111755371094,
    15: 0.06143593788146973,
    16: 0.06163835525512695,
    17: 0.057886600494384766,
    # 18: 0.8943188190460205,
    19: 0.057190656661987305,
    20: 0.05732560157775879,
    21: 0.05961871147155762,
    22: 0.06056499481201172,
    23: 0.06168651580810547,
    24: 0.06287384033203125,
    25: 0.06479954719543457,
    26: 0.06631922721862793,
    27: 0.06624102592468262,
    28: 0.06885766983032227,
    29: 0.07118654251098633,
    30: 0.07218647003173828,
    31: 0.07354331016540527,
    32: 0.07480692863464355,
    33: 0.07707786560058594,
    34: 0.07780647277832031,
    35: 0.07924175262451172,
    36: 0.08091855049133301,
    37: 0.08162164688110352,
    38: 0.08273458480834961,
    39: 0.08522272109985352,
    40: 0.08962702751159668,
    41: 0.0878913402557373,
    42: 0.09065818786621094,
    43: 0.09179115295410156,
    44: 0.0926506519317627,
    45: 0.09462475776672363,
    46: 0.0958399772644043,
    48: 0.1001279354095459,
    49: 0.09983706474304199,
    50: 0.10154485702514648,
    51: 0.10113382339477539
}

performer_inferences3 = {
    10: 0.04698467254638672,
    11: 0.0499727725982666,
    12: 0.05454206466674805,
    13: 0.052930355072021484,
    14: 0.0540468692779541,
    15: 0.05763602256774902,
    16: 0.06031012535095215,
    17: 0.05773758888244629,
    # 18: 0.8953473567962646,
    19: 0.056943655014038086,
    20: 0.05720233917236328,
    21: 0.05957293510437012,
    22: 0.06022047996520996,
    23: 0.06276583671569824,
    24: 0.06263518333435059,
    25: 0.06461286544799805,
    26: 0.06622076034545898,
    27: 0.06846332550048828,
    28: 0.06902241706848145,
    29: 0.07114577293395996,
    30: 0.07213926315307617,
    31: 0.07348227500915527,
    32: 0.07487940788269043,
    33: 0.0772249698638916,
    34: 0.0779266357421875,
    35: 0.07933688163757324,
    36: 0.0808565616607666,
    37: 0.08178973197937012,
    38: 0.08278894424438477,
    39: 0.08507657051086426,
    40: 0.08624625205993652,
    41: 0.08774113655090332,
    42: 0.0906229019165039,
    43: 0.09168386459350586,
    44: 0.09248852729797363,
    45: 0.09441041946411133,
    46: 0.09584569931030273,
    48: 0.09722304344177246,
    49: 0.09837150573730469,
    50: 0.10126447677612305,
    51: 0.10343813896179199
}

bigbird_inferences1 = {
    10: 0.06829190254211426,
    11: 0.07210540771484375,
    12: 0.06725907325744629,
    13: 0.07325243949890137,
    14: 0.07448148727416992,
    15: 0.07395124435424805,
    16: 0.0757150650024414,
    17: 0.07570433616638184,
    # 18: 0.953228235244751,
    19: 0.07775115966796875,
    20: 0.07759284973144531,
    21: 0.07782602310180664,
    22: 0.07945632934570312,
    23: 0.07993316650390625,
    24: 0.08675479888916016,
    25: 0.08792972564697266,
    26: 0.08866524696350098,
    27: 0.08935165405273438,
    28: 0.09382510185241699,
    29: 0.09098696708679199,
    30: 0.09073948860168457,
    31: 0.09333181381225586,
    32: 0.10935449600219727,
    33: 0.09752202033996582,
    34: 0.08834338188171387,
    35: 0.08857846260070801,
    36: 0.08878540992736816,
    37: 0.09459996223449707,
    38: 0.0906825065612793,
    39: 0.09165143966674805,
    40: 0.09864425659179688,
    41: 0.09879517555236816,
    42: 0.09480643272399902,
    43: 0.0979771614074707,
    44: 0.10240912437438965,
    45: 0.09818267822265625,
    46: 0.10363078117370605,
    48: 0.10150551795959473,
    49: 0.10466790199279785,
    50: 0.10733938217163086,
    51: 0.1082301139831543
}

bigbird_inferences2 = {
    10: 0.07253885269165039,
    11: 0.0736391544342041,
    12: 0.07227826118469238,
    13: 0.0798807144165039,
    14: 0.07990264892578125,
    15: 0.0796353816986084,
    16: 0.08222723007202148,
    17: 0.08250927925109863,
    # 18: 0.9986498355865479,
    19: 0.08270978927612305,
    20: 0.08441162109375,
    21: 0.08327078819274902,
    22: 0.08446002006530762,
    23: 0.08500194549560547,
    24: 0.08472847938537598,
    25: 0.09403467178344727,
    26: 0.09511399269104004,
    27: 0.09260034561157227,
    28: 0.09790849685668945,
    29: 0.10083150863647461,
    30: 0.0990755558013916,
    31: 0.10206103324890137,
    32: 0.10535526275634766,
    33: 0.10240578651428223,
    34: 0.09576153755187988,
    35: 0.0969851016998291,
    36: 0.09553027153015137,
    37: 0.09796714782714844,
    38: 0.09793663024902344,
    39: 0.09849309921264648,
    40: 0.10187768936157227,
    41: 0.10073208808898926,
    42: 0.10262513160705566,
    43: 0.10562634468078613,
    44: 0.10426950454711914,
    45: 0.10526871681213379,
    46: 0.11015629768371582,
    48: 0.10840129852294922,
    49: 0.11053895950317383,
    50: 0.11199188232421875,
    51: 0.11071419715881348
}

bigbird_inferences3 = {
    10: 0.05961275100708008,
    11: 0.058423519134521484,
    12: 0.05726432800292969,
    13: 0.0689239501953125,
    14: 0.06479573249816895,
    15: 0.06435894966125488,
    16: 0.06703448295593262,
    17: 0.06704902648925781,
    # 18: 0.9388813972473145, 
    19: 0.06779026985168457,
    20: 0.0677497386932373,
    21: 0.06758594512939453,
    22: 0.06838726997375488,
    23: 0.06885814666748047,
    24: 0.07159638404846191,
    25: 0.07726263999938965,
    26: 0.07731270790100098,
    27: 0.07663154602050781,
    28: 0.08025598526000977,
    29: 0.08089685440063477,
    30: 0.08426046371459961,
    31: 0.08324766159057617,
    32: 0.08376073837280273,
    33: 0.08277153968811035,
    34: 0.07505083084106445,
    35: 0.07589983940124512,
    36: 0.07523608207702637,
    37: 0.07657980918884277,
    38: 0.07659268379211426,
    39: 0.07617759704589844,
    40: 0.07806801795959473,
    41: 0.08188295364379883,
    42: 0.07755661010742188,
    43: 0.0803828239440918,
    44: 0.07973027229309082,
    45: 0.07917141914367676,
    46: 0.08440661430358887,
    48: 0.08081984519958496,
    49: 0.08222770690917969,
    50: 0.08336305618286133,
    51: 0.08169245719909668
}

transformer_x = list(transformer_inferences1.keys())
transformer_y1 = list(transformer_inferences1.values())
transformer_y2 = list(transformer_inferences2.values())
transformer_y3 = list(transformer_inferences3.values())

transformer_y = []
for y1, y2, y3 in zip(transformer_y1, transformer_y2, transformer_y3):
    transformer_y.append([y1, y2, y3])
transformer_y = np.asarray(transformer_y)
transformer_y_avg = transformer_y.mean(axis=1)
transformer_y_std = transformer_y.std(axis=1)

# print (transformer_y_avg)
# print (sum([0.02037954330444336, 0.020636796951293945, 0.020218849182128906]) / 3)
# print (transformer_y_avg[0])

performer_x = list(performer_inferences1.keys())
performer_y1 = list(performer_inferences1.values())
performer_y2 = list(performer_inferences2.values())
performer_y3 = list(performer_inferences3.values())

performer_y = []
for y1, y2, y3 in zip(performer_y1, performer_y2, performer_y3):
    performer_y.append([y1, y2, y3])
performer_y = np.asarray(performer_y)
performer_y_avg = performer_y.mean(axis=1)
performer_y_std = performer_y.std(axis=1)

bigbird_x = list(bigbird_inferences1.keys())
bigbird_y1 = list(bigbird_inferences1.values())
bigbird_y2 = list(bigbird_inferences2.values())
bigbird_y3 = list(bigbird_inferences3.values()) 

bigbird_y = []
for y1, y2, y3 in zip(bigbird_y1, bigbird_y2, bigbird_y3):
    bigbird_y.append([y1, y2, y3])
bigbird_y = np.asarray(bigbird_y)
bigbird_y_avg = bigbird_y.mean(axis=1)
bigbird_y_std = bigbird_y.std(axis=1)

fig = plt.figure()
ax = fig.add_subplot(111)

# xint = range(min(transformer_x), math.ceil(max(transformer_x))+1)
# matplotlib.pyplot.xticks(xint)

plt.plot(transformer_x, transformer_y_avg, color="red", label="GPS-Transformer-M", markersize=10)
plt.fill_between(
    transformer_x,
    np.asarray(transformer_y_avg) - np.asarray(transformer_y_std), 
    np.asarray(transformer_y_avg) + np.asarray(transformer_y_std),
    alpha=0.3,
    color="red"
)

plt.plot(performer_x, performer_y_avg, color="blue", label="GPS-Performer-M", markersize=10)
plt.fill_between(
    performer_x,
    np.asarray(performer_y_avg) - np.asarray(performer_y_std),
    np.asarray(performer_y_avg) + np.asarray(performer_y_std),
    alpha=0.3,
    color="blue"
)

plt.plot(bigbird_x, bigbird_y_avg, color="green", label="GPS-BigBird-M")
plt.fill_between(
    bigbird_x,
    np.asarray(bigbird_y_avg) - np.asarray(bigbird_y_std),
    np.asarray(bigbird_y_avg) + np.asarray(bigbird_y_std),
    alpha=0.3,
    color="green"
)

plt.xlabel("Number of atoms")
plt.ylabel("Batch Inference Runtime (B = 128) / sec")
plt.legend()
plt.grid(linestyle="dashed")
plt.show()

fig.savefig("figures/scaling/pcqm4m_variant_scaling_graph_size.pdf", dpi=400, bbox_inches='tight')